{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8166995,"sourceType":"datasetVersion","datasetId":4832834}],"dockerImageVersionId":30702,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Reading the articles","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize an empty list to store the articles\narticles = []\n\n# List of file names \nl = [1,2,3,7,8,9,11,12,13,14,15,16,17,18,21,22,23,24,25,26]\n\n# Loop through each file name\nfor i in l:\n    # Open each file in read mode with latin-1 encoding\n    f = open(rf\"/kaggle/input/papers/ResearchPapers/{i}.txt\" , encoding='latin-1')\n    \n    # Read the content of the file\n    s = f.read()\n    \n    # Append the content to the articles list\n    articles.append(s)\n\n# Print the total number of articles read\nprint(len(articles))\n\n# List of stopwords to be excluded from the analysis\nstopwords = ['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:10.167072Z","iopub.execute_input":"2024-04-19T16:03:10.167464Z","iopub.status.idle":"2024-04-19T16:03:10.219018Z","shell.execute_reply.started":"2024-04-19T16:03:10.167435Z","shell.execute_reply":"2024-04-19T16:03:10.217861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprossecing","metadata":{}},{"cell_type":"code","source":"import re  # Importing the regular expression module\nfrom nltk.stem.porter import PorterStemmer  # Importing Porter Stemmer from NLTK\n\nport_stem = PorterStemmer()  # Initializing a Porter Stemmer object\n\n# Function to perform stemming on content\ndef stemming(content):\n    stemmed_content = re.sub('[^\\w\\s]', '', content)  # Removing non-alphanumeric characters except whitespace\n    stemmed_content = stemmed_content.lower()  # Converting content to lowercase\n    stemmed_content = stemmed_content.split()  # Splitting content into tokens\n    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords]  # Stemming and removing stopwords\n    return stemmed_content  # Returning the stemmed content\n\ntemp = []  # Initializing an empty list for temporary storage\n\n# Looping through each article in the list of articles\nfor i in articles:\n    # Applying stemming to the current article and appending the result to the temporary list\n    temp.append(stemming(i))\n\n# Assigning the stemmed articles back to the original articles list\narticles = temp\n\n# Printing the stemmed articles\nprint(articles[1])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:10.588458Z","iopub.execute_input":"2024-04-19T16:03:10.589867Z","iopub.status.idle":"2024-04-19T16:03:16.963099Z","shell.execute_reply.started":"2024-04-19T16:03:10.589813Z","shell.execute_reply":"2024-04-19T16:03:16.96179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = set()  # Initialize an empty set to store unique words (vocabulary)\n\n# Iterate through each article in the list of articles\nfor i in articles:\n    # Update the vocabulary set with the words in the current article\n    vocab.update(set(i))\n\n# Print the length of the vocabulary (number of unique words)\nprint(len(vocab))\n\n# Sort the vocabulary set alphabetically and convert it back to a list\nvocab = sorted(vocab)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:16.965702Z","iopub.execute_input":"2024-04-19T16:03:16.966329Z","iopub.status.idle":"2024-04-19T16:03:17.006587Z","shell.execute_reply.started":"2024-04-19T16:03:16.966286Z","shell.execute_reply":"2024-04-19T16:03:17.005438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF from scratch\ncredit : https://www.kaggle.com/code/yassinehamdaoui1/creating-tf-idf-model-from-scratch","metadata":{}},{"cell_type":"markdown","source":"### TF","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\n\n# Initialize an empty list to store dictionaries\ndicts = []\n\n# Loop through each article in the list of articles\nfor article in articles:\n    # Count occurrences of each word in the current article using Counter\n    word_counts = Counter(article)\n    \n    # Create a dictionary with keys from the vocabulary and values from word_counts\n    dic = {word: word_counts.get(word, 0) for word in vocab}\n    \n    # Append the dictionary for the current article to the list\n    dicts.append(dic)\n\n# Create a DataFrame from the list of dictionaries\ndf = pd.DataFrame(dicts)\n\n# Print or further process the DataFrame as needed\ndf\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:17.008136Z","iopub.execute_input":"2024-04-19T16:03:17.008594Z","iopub.status.idle":"2024-04-19T16:03:18.458904Z","shell.execute_reply.started":"2024-04-19T16:03:17.008554Z","shell.execute_reply":"2024-04-19T16:03:18.457888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to compute term frequency (TF) scores\ndef computeTF(wordDict, doc):\n    tfDict = {}  # Initialize an empty dictionary for TF scores\n    corpusCount = len(doc)  # Count of words in the document (corpus)\n    \n    # Calculate TF score for each word in the wordDict\n    for word, count in wordDict.items():\n        tfDict[word] = count / float(corpusCount)  # TF = (Number of occurrences of word) / (Total number of words)\n    \n    return tfDict  # Return the dictionary of TF scores\n\ntfscores = []  # Initialize an empty list to store TF scores for each article\n\n# Loop through each dictionary (word counts) and article pair using zip\nfor dic, article in zip(dicts, articles):\n    # Compute TF scores for the current article using the word counts dictionary\n    tfscores.append(computeTF(dic, article))\n\n# Create a DataFrame from the list of TF scores\ntf = pd.DataFrame(tfscores)\n\n# Print or further process the DataFrame as needed\ntf\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:18.461313Z","iopub.execute_input":"2024-04-19T16:03:18.461896Z","iopub.status.idle":"2024-04-19T16:03:19.411523Z","shell.execute_reply.started":"2024-04-19T16:03:18.461863Z","shell.execute_reply":"2024-04-19T16:03:19.410309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### IDF","metadata":{}},{"cell_type":"code","source":"import math  # Importing the math module for mathematical operations\n\n# Function to compute Inverse Document Frequency (IDF) scores\ndef computeIDF(docList):\n    idfDict = {}  # Initialize an empty dictionary to store IDF scores\n    N = len(docList)  # Total number of documents in the corpus\n    \n    # Initialize idfDict with keys from the first document's keys and values set to 0\n    idfDict = dict.fromkeys(docList[0].keys(), 0)\n    \n    # Calculate IDF score for each word in idfDict\n    for word, val in idfDict.items():\n        # Compute IDF score using the formula: IDF = log(N / (df + 1))\n        # where N is the total number of documents and df is the document frequency of the word\n        idfDict[word] = math.log10(N / (float(val) + 1))\n        \n    return idfDict  # Return the dictionary containing IDF scores\n\n# Compute IDF scores for the list of dictionaries (word counts for each document)\nidfs = computeIDF(dicts)\n\n# Further process or use the IDF scores as needed\nprint(idfs)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:19.412936Z","iopub.execute_input":"2024-04-19T16:03:19.413311Z","iopub.status.idle":"2024-04-19T16:03:19.495462Z","shell.execute_reply.started":"2024-04-19T16:03:19.413281Z","shell.execute_reply":"2024-04-19T16:03:19.493816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF","metadata":{}},{"cell_type":"code","source":"def computeTFIDF(tfBow, idfs):\n    tfidf = {}  # Initialize an empty dictionary to store TF-IDF scores\n    for word, val in tfBow.items():\n        # Compute TF-IDF score for each word using TF score and IDF score\n        tfidf[word] = val * idfs[word]\n    return tfidf  # Return the dictionary containing TF-IDF scores\n\n# Iterate through each row in the TF DataFrame and compute TF-IDF scores\n\nTFIDF = []\nfor tfrow in tf.iterrows():\n    # Extract the row (TF scores for one article) as a dictionary\n    tfBow = dict(tfrow[1])\n    # Compute TF-IDF scores for the current article using TF scores and IDF scores\n    TFIDF.append(computeTFIDF(tfBow, idfs))\n    \n# Create a DataFrame to store the IDF scores\ntf_idf = pd.DataFrame(TFIDF)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:19.497108Z","iopub.execute_input":"2024-04-19T16:03:19.49742Z","iopub.status.idle":"2024-04-19T16:03:22.833961Z","shell.execute_reply.started":"2024-04-19T16:03:19.497394Z","shell.execute_reply":"2024-04-19T16:03:22.832847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:22.835244Z","iopub.execute_input":"2024-04-19T16:03:22.835916Z","iopub.status.idle":"2024-04-19T16:03:22.882288Z","shell.execute_reply.started":"2024-04-19T16:03:22.835883Z","shell.execute_reply":"2024-04-19T16:03:22.881066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Query prcocessing","metadata":{}},{"cell_type":"code","source":"def process_query(query):\n\n    query = stemming(query)  # Preprocess the query: tokenize and stem\n\n    # Add the preprocessed query to the list of articles\n    temp = articles.copy()  # Create a shallow copy of the articles list\n    temp.append(query)  # Add the query to the temp list\n\n\n    query_added_articles_list = temp\n\n    tf_dicts = []  # List to store TF scores for each article\n\n    # Calculate TF scores for each article in the combined list\n    for article in query_added_articles_list:\n        word_counts = Counter(article)  # Count occurrences of each word\n        # Create a dictionary with word counts for each word in the vocabulary\n        dic = {word: word_counts.get(word, 0) for word in vocab}\n        tf_dicts.append(dic)  # Append the TF dictionary for the current article\n\n    tfscores = []  # List to store TF scores\n\n    # Compute TF scores for each article\n    for dic, article in zip(tf_dicts, query_added_articles_list):\n        tfscores.append(computeTF(dic, query_added_articles_list))\n\n    # Create a DataFrame to store TF scores\n    tf = pd.DataFrame(tfscores)\n\n    # Compute IDF scores for the list of TF dictionaries\n    idfs = computeIDF(tf_dicts)\n\n    TFIDF = []  # List to store TF-IDF scores\n\n    # Compute TF-IDF scores for each TF score dictionary\n    for tfs in tfscores:\n        TFIDF.append(computeTFIDF(tfs, idfs))\n\n    # Create a DataFrame to store TF-IDF scores\n    return TFIDF\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:22.884218Z","iopub.execute_input":"2024-04-19T16:03:22.884667Z","iopub.status.idle":"2024-04-19T16:03:22.895113Z","shell.execute_reply.started":"2024-04-19T16:03:22.884611Z","shell.execute_reply":"2024-04-19T16:03:22.893852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cosine Similarity","metadata":{}},{"cell_type":"code","source":"# Function to calculate the dot product of two dictionaries\ndef dot_product(dict1, dict2):\n    # Get the keys that are common to both dictionaries\n    common_keys = set(dict1.keys()) & set(dict2.keys())\n    \n    # Initialize the dot product to 0\n    dot_product = 0\n    \n    # Iterate through the common keys and calculate the dot product\n    for key in common_keys:\n        dot_product += dict1[key] * dict2[key]\n    \n    return dot_product\n\n# Function to calculate the magnitude of a dictionary (treated as a vector)\ndef magnitude(dictionary):\n    # Calculate the sum of the squares of the values\n    sum_of_squares = sum(value ** 2 for value in dictionary.values())\n    \n    # Take the square root of the sum\n    mag = math.sqrt(sum_of_squares)\n    \n    return mag\n\ndef compute_similarity(TFIDF):\n    # Extract the TF-IDF vector for the query from the list of TF-IDF vectors\n    query_vec = TFIDF.pop()\n\n    # List to store similarity scores between the query vector and other vectors\n    similarity = []\n\n    # Calculate similarity scores between the query vector and each remaining TF-IDF vector\n    for V in TFIDF:\n        # Calculate dot product between the query vector and the current TF-IDF vector\n        dp = dot_product(V, query_vec)\n        \n        # Calculate magnitudes of the two vectors\n        m1 = magnitude(V)\n        m2 = magnitude(query_vec)\n\n        # Calculate cosine similarity between the two vectors\n        if m1*m2 == 0:\n            sim = -1\n        else:\n            sim = dp / (m1 * m2)\n        if (sim > 0.001):\n        # Append the similarity score to the list\n            similarity.append(sim)\n        else:\n            similarity.append(-1)\n            \n    indexes = list(range(len(similarity)))\n\n    # Filter out indexes where the similarity score is -1\n    filtered_indexes = [index for index, sim in zip(indexes, similarity) if sim != -1]\n\n    # Sort the filtered indexes based on the similarity scores (in descending order)\n    sorted_indexes = sorted(filtered_indexes, key=lambda x: similarity[x], reverse=True)\n    sorted_indexes = [x + 1 for x in sorted_indexes]\n    # Print the sorted indexes\n    return (sorted_indexes , similarity)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:22.896878Z","iopub.execute_input":"2024-04-19T16:03:22.897296Z","iopub.status.idle":"2024-04-19T16:03:22.912545Z","shell.execute_reply.started":"2024-04-19T16:03:22.89725Z","shell.execute_reply":"2024-04-19T16:03:22.911689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = 'Machine learing'\ntfidf = process_query(test)\nquery_results, similarity_scores = compute_similarity(tfidf)\nprint(query_results)\n\n\n# The formula to calculate cosine similarity between two vectors A and B is as follows:\n\n# cosine_similarity(A, B) = (A · B) / (||A|| * ||B||)\n\n# Where:\n\n# (A · B) represents the dot product of vectors A and B.\n# ||A|| represents the magnitude (or Euclidean norm) of vector A, calculated as the square root of the sum of the squares of its components.\n# ||B|| represents the magnitude of vector B, calculated in the same way.\n# The cosine similarity ranges from -1 to 1:\n\n# If the cosine similarity is 1, it means the vectors are pointing in the same direction and are identical.\n# If the cosine similarity is -1, it means the vectors are pointing in opposite directions and are completely dissimilar.\n# If the cosine similarity is 0, it means the vectors are orthogonal (perpendicular) and have no similarity.\n# In the context of the provided code, cosine similarity is used to measure the similarity between the query vector and the TF-IDF vectors. The similarity scores are calculated using the cosine similarity formula, and the higher the score, the more similar the vectors are considered to be.","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:03:22.915071Z","iopub.execute_input":"2024-04-19T16:03:22.915929Z","iopub.status.idle":"2024-04-19T16:03:24.748698Z","shell.execute_reply.started":"2024-04-19T16:03:22.915897Z","shell.execute_reply":"2024-04-19T16:03:24.747478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  GUI","metadata":{}},{"cell_type":"code","source":"import tkinter as tk\nfrom tkinter import ttk\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nimport matplotlib.pyplot as plt\n\ndef calculate_and_display():\n    # Get the input string from the entry widget\n    input_string = entry.get()\n    \n    # Call your functions with the input string\n    tfidf = process_query(input_string)\n    query_results, similarity_scores = compute_similarity(tfidf)\n    \n    # Display the results on the GUI\n    result_label.config(text=f\"Query Result: {query_results}\")\n    \n    # Plot the similarity scores\n    plot_similarity(similarity_scores)\n\ndef plot_similarity(similarity_scores):\n    # Clear the previous plot if any\n    for widget in result_frame.winfo_children():\n        widget.destroy()\n    \n    # Create a new figure for the plot\n    fig = plt.Figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    \n    # Plot the similarity scores\n    l = range(1, len(similarity_scores) + 1)\n    bars = ax.bar(l, similarity_scores, color=['salmon' if score < 0 else 'skyblue' for score in similarity_scores])\n    ax.set_xlabel('Document Index')  # Label for the x-axis\n    ax.set_ylabel('Similarity Score')  # Label for the y-axis\n    ax.set_title('Similarity Scores for Documents')  # Title of the plot\n    ax.set_xticks(l)  # Set the x-ticks to document indexes\n    \n    # Embed the Matplotlib plot in the Tkinter window\n    canvas = FigureCanvasTkAgg(fig, master=result_frame)\n    canvas.draw()\n    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n\n# Create the main window\nroot = tk.Tk()\nroot.title(\"TF-IDF Search Engine\")\n\n# Styling\nroot.geometry(\"500x400\")\nroot.configure(bg=\"#f0f0f0\")\n\n# Create a label for the input\ninput_label = ttk.Label(root, text=\"Enter your search query:\", background=\"#f0f0f0\", font=(\"Arial\", 12))\ninput_label.pack(pady=10)\n\n# Create an entry widget for the input\nentry = ttk.Entry(root, width=40, font=(\"Arial\", 10))\nentry.pack(pady=5)\n\n# Create a button to calculate and display the results\ncalculate_button = ttk.Button(root, text=\"Search\", command=calculate_and_display)\ncalculate_button.pack(pady=5)\n\n# Create a frame to contain the plot\nresult_frame = ttk.Frame(root)\nresult_frame.pack(pady=10, padx=10, fill=tk.BOTH, expand=True)\n\n# Create a label to display the results\nresult_label = ttk.Label(root, text=\"\", background=\"#f0f0f0\", font=(\"Arial\", 10, \"italic\"), wraplength=380)\nresult_label.pack(pady=10)\n\n# Run the Tkinter event loop\nroot.mainloop()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}